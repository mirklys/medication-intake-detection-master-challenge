{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":31040,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":5,"nbformat":4,"cells":[{"id":"ab21d24a","cell_type":"code","source":"from transformers import SiglipForImageClassification, AutoImageProcessor\nimport os\nimport pandas as pd\nfrom PIL import Image\nfrom datasets import Dataset\nfrom transformers import Trainer, TrainingArguments\nimport numpy as np\nfrom sklearn.metrics import accuracy_score, precision_recall_fscore_support\nimport torch","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-16T20:10:02.371343Z","iopub.execute_input":"2025-05-16T20:10:02.371642Z","iopub.status.idle":"2025-05-16T20:10:44.901533Z","shell.execute_reply.started":"2025-05-16T20:10:02.371615Z","shell.execute_reply":"2025-05-16T20:10:44.900125Z"}},"outputs":[{"name":"stderr","text":"2025-05-16 20:10:23.520675: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1747426223.869677      35 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1747426223.965441      35 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"}],"execution_count":1},{"id":"b3440522","cell_type":"code","source":"! apt-get install -y gdown","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-16T20:10:44.903982Z","iopub.execute_input":"2025-05-16T20:10:44.904787Z","iopub.status.idle":"2025-05-16T20:10:48.240674Z","shell.execute_reply.started":"2025-05-16T20:10:44.904753Z","shell.execute_reply":"2025-05-16T20:10:48.239413Z"}},"outputs":[{"name":"stdout","text":"Reading package lists... Done\nBuilding dependency tree... Done\nReading state information... Done\nE: Unable to locate package gdown\n","output_type":"stream"}],"execution_count":2},{"id":"476daad5-a99d-4678-a522-c51345749867","cell_type":"code","source":"# download zip from google drive and unzip contents\n! gdown --id 1rbISVuHbT_AJPw4wv4ywLym3TLVuGLDH      ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-16T20:10:48.245085Z","iopub.execute_input":"2025-05-16T20:10:48.245434Z","iopub.status.idle":"2025-05-16T20:11:03.386964Z","shell.execute_reply.started":"2025-05-16T20:10:48.245392Z","shell.execute_reply":"2025-05-16T20:11:03.385775Z"}},"outputs":[{"name":"stdout","text":"/usr/local/lib/python3.11/dist-packages/gdown/__main__.py:140: FutureWarning: Option `--id` was deprecated in version 4.3.1 and will be removed in 5.0. You don't need to pass it anymore to use a file ID.\n  warnings.warn(\nDownloading...\nFrom (original): https://drive.google.com/uc?id=1rbISVuHbT_AJPw4wv4ywLym3TLVuGLDH\nFrom (redirected): https://drive.google.com/uc?id=1rbISVuHbT_AJPw4wv4ywLym3TLVuGLDH&confirm=t&uuid=74640b90-3dca-40e9-9600-6ce3477a6ecc\nTo: /kaggle/working/frames_and_annotations.zip\n100%|███████████████████████████████████████| 1.14G/1.14G [00:10<00:00, 113MB/s]\n","output_type":"stream"}],"execution_count":3},{"id":"76737129-e66d-45ee-84ca-65586cd6438d","cell_type":"code","source":"# Unzip the file\nimport zipfile\nwith zipfile.ZipFile(\"./frames_and_annotations.zip\", \"r\") as zip_ref:\n    zip_ref.extractall(\"frames\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-16T20:11:03.388231Z","iopub.execute_input":"2025-05-16T20:11:03.388537Z","iopub.status.idle":"2025-05-16T20:11:12.532323Z","shell.execute_reply.started":"2025-05-16T20:11:03.388504Z","shell.execute_reply":"2025-05-16T20:11:12.531136Z"}},"outputs":[],"execution_count":4},{"id":"1b77880b","cell_type":"code","source":"model = SiglipForImageClassification.from_pretrained(\"prithivMLmods/Human-Action-Recognition\")\nprocessor = AutoImageProcessor.from_pretrained(\"prithivMLmods/Human-Action-Recognition\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-16T20:11:12.534050Z","iopub.execute_input":"2025-05-16T20:11:12.534674Z","iopub.status.idle":"2025-05-16T20:11:16.414512Z","shell.execute_reply.started":"2025-05-16T20:11:12.534620Z","shell.execute_reply":"2025-05-16T20:11:16.413681Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/1.66k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"02a8f2b7880c4b3d963bff709f65b6ec"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/372M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5e19f8dbba3d42a1bce1b4d36c6edc16"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"preprocessor_config.json:   0%|          | 0.00/394 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9490affd52824ab99ce6f297d8abbcb0"}},"metadata":{}},{"name":"stderr","text":"Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n","output_type":"stream"}],"execution_count":5},{"id":"6a0ad2c1","cell_type":"code","source":"df = pd.read_csv(\"./frames/frames_annotations.csv\")\n\n# fix backslash to forward slash in relative paths\ndf[\"image\"] = df[\"image\"].apply(lambda x: os.path.join(\"frames\", x.replace(\"\\\\\", \"/\")))\n\n#assign labels\ndf[\"label\"] = df[\"label\"].map({\"not_taking_medication\": 0, \"taking_medication\": 1})\n\n# generate dataset from df\ndataset = Dataset.from_pandas(df)\n\ndef preprocess(example):\n    image = Image.open(example[\"image\"]).convert(\"RGB\")\n    inputs = processor(images=image, return_tensors=\"pt\")\n    inputs[\"labels\"] = example[\"label\"]\n    return {\n        \"pixel_values\": inputs[\"pixel_values\"].squeeze(),\n        \"labels\": inputs[\"labels\"]\n    }\n\ndataset = dataset.map(preprocess, remove_columns=[\"image\", \"label\"])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-16T20:11:16.416483Z","iopub.execute_input":"2025-05-16T20:11:16.416761Z","iopub.status.idle":"2025-05-16T20:12:11.091083Z","shell.execute_reply.started":"2025-05-16T20:11:16.416737Z","shell.execute_reply":"2025-05-16T20:12:11.090199Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/850 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d03d60e81f3941199d9567c919e9f327"}},"metadata":{}}],"execution_count":6},{"id":"97101522","cell_type":"code","source":"dataset = dataset.train_test_split(test_size=0.1)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-16T13:30:57.418376Z","iopub.execute_input":"2025-05-16T13:30:57.419790Z","iopub.status.idle":"2025-05-16T13:30:57.439872Z","shell.execute_reply.started":"2025-05-16T13:30:57.419743Z","shell.execute_reply":"2025-05-16T13:30:57.438800Z"}},"outputs":[],"execution_count":7},{"id":"02a5111d","cell_type":"code","source":"def compute_metrics(eval_pred):\n    logits, labels = eval_pred\n    # Ensure logits are numpy arrays\n    if isinstance(logits, tuple):\n        print(\"logits is tuple\")\n        logits = logits[0]\n    if isinstance(logits, torch.Tensor):\n        print(\"logits is tensor\")\n        logits = logits.detach().cpu().numpy()\n    if isinstance(labels, torch.Tensor):\n        print(\"labels is tensor\")\n        labels = labels.detach().cpu().numpy()\n    preds = np.argmax(logits, axis=1)\n    acc = accuracy_score(labels, preds)\n    precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average=\"binary\", zero_division=0)\n    return {\n        \"accuracy\": acc,\n        \"precision\": precision,\n        \"recall\": recall,\n        \"f1\": f1,\n    }","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-16T14:26:49.086931Z","iopub.execute_input":"2025-05-16T14:26:49.087335Z","iopub.status.idle":"2025-05-16T14:26:49.095511Z","shell.execute_reply.started":"2025-05-16T14:26:49.087309Z","shell.execute_reply":"2025-05-16T14:26:49.094194Z"}},"outputs":[],"execution_count":18},{"id":"2030c48a","cell_type":"code","source":"model.config.label2id = {\"not_taking_medication\": 0, \"taking_medication\": 1}\nmodel.config.id2label = {0: \"not_taking_medication\", 1: \"taking_medication\"}\n\ntraining_args = TrainingArguments(\n    output_dir=\"./HAR-medication-finetuned\",\n    per_device_train_batch_size=8,\n    per_device_eval_batch_size=8,\n    eval_strategy=\"epoch\",\n    save_strategy=\"epoch\",\n    num_train_epochs=3,\n    logging_dir=\"./logs\",\n    logging_steps=10,\n    hub_model_id=\"Adekiii/HAR-medication-finetuned\",\n    report_to=[\"none\"],\n)\n\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=dataset[\"train\"],\n    eval_dataset=dataset[\"test\"],\n    compute_metrics=compute_metrics,\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-16T13:32:23.780126Z","iopub.execute_input":"2025-05-16T13:32:23.780449Z","iopub.status.idle":"2025-05-16T13:32:23.801399Z","shell.execute_reply.started":"2025-05-16T13:32:23.780425Z","shell.execute_reply":"2025-05-16T13:32:23.800316Z"}},"outputs":[],"execution_count":12},{"id":"a5a779f6-8731-4422-bb63-3c71d239a34f","cell_type":"code","source":"# Use GPU if available\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel.to(device)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-16T14:26:55.070509Z","iopub.execute_input":"2025-05-16T14:26:55.070889Z","iopub.status.idle":"2025-05-16T14:26:55.085913Z","shell.execute_reply.started":"2025-05-16T14:26:55.070862Z","shell.execute_reply":"2025-05-16T14:26:55.084489Z"}},"outputs":[{"execution_count":19,"output_type":"execute_result","data":{"text/plain":"SiglipForImageClassification(\n  (vision_model): SiglipVisionTransformer(\n    (embeddings): SiglipVisionEmbeddings(\n      (patch_embedding): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16), padding=valid)\n      (position_embedding): Embedding(196, 768)\n    )\n    (encoder): SiglipEncoder(\n      (layers): ModuleList(\n        (0-11): 12 x SiglipEncoderLayer(\n          (layer_norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n          (self_attn): SiglipAttention(\n            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n          )\n          (layer_norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n          (mlp): SiglipMLP(\n            (activation_fn): PytorchGELUTanh()\n            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n          )\n        )\n      )\n    )\n    (post_layernorm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n    (head): SiglipMultiheadAttentionPoolingHead(\n      (attention): MultiheadAttention(\n        (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n      )\n      (layernorm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n      (mlp): SiglipMLP(\n        (activation_fn): PytorchGELUTanh()\n        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n      )\n    )\n  )\n  (classifier): Linear(in_features=768, out_features=15, bias=True)\n)"},"metadata":{}}],"execution_count":19},{"id":"263c6e9d","cell_type":"code","source":"trainer.train()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-16T14:26:59.406101Z","iopub.execute_input":"2025-05-16T14:26:59.406705Z","iopub.status.idle":"2025-05-16T15:12:05.127157Z","shell.execute_reply.started":"2025-05-16T14:26:59.406536Z","shell.execute_reply":"2025-05-16T15:12:05.125933Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='288' max='288' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [288/288 44:55, Epoch 3/3]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Accuracy</th>\n      <th>Precision</th>\n      <th>Recall</th>\n      <th>F1</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>0.455900</td>\n      <td>0.292818</td>\n      <td>0.905882</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>0.539300</td>\n      <td>0.363245</td>\n      <td>0.905882</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>0.334300</td>\n      <td>0.287595</td>\n      <td>0.905882</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n","output_type":"stream"},{"execution_count":20,"output_type":"execute_result","data":{"text/plain":"TrainOutput(global_step=288, training_loss=0.5068342975444264, metrics={'train_runtime': 2705.1499, 'train_samples_per_second': 0.848, 'train_steps_per_second': 0.106, 'total_flos': 1.935798167938683e+17, 'train_loss': 0.5068342975444264, 'epoch': 3.0})"},"metadata":{}}],"execution_count":20},{"id":"0aa83ffa","cell_type":"code","source":"trainer.save_model(\"./HAR-med-finetunedv2\")\nprocessor.save_pretrained(\"./HAR-med-finetunedv2\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-16T14:18:15.413040Z","iopub.execute_input":"2025-05-16T14:18:15.414539Z","iopub.status.idle":"2025-05-16T14:18:16.606962Z","shell.execute_reply.started":"2025-05-16T14:18:15.414495Z","shell.execute_reply":"2025-05-16T14:18:16.605885Z"}},"outputs":[{"execution_count":14,"output_type":"execute_result","data":{"text/plain":"['./HAR-med-finetuned/preprocessor_config.json']"},"metadata":{}}],"execution_count":14},{"id":"c01f732a","cell_type":"code","source":"from huggingface_hub import HfApi\n\naccess_token = \"...\"\n\napi = HfApi(token=access_token)\napi.upload_folder(\n    folder_path=\"HAR-med-finetunedv2\",\n    repo_id=\"tam6/HAR-medication-finetunedv2\", # change with own repo\n    repo_type=\"model\",\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-16T14:18:19.940912Z","iopub.execute_input":"2025-05-16T14:18:19.941218Z","iopub.status.idle":"2025-05-16T14:18:36.801077Z","shell.execute_reply.started":"2025-05-16T14:18:19.941199Z","shell.execute_reply":"2025-05-16T14:18:36.799974Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/372M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"55bde5d192264022b01f258a149dc5af"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Upload 2 LFS files:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e47147a2992949db92316d057f059dbd"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"training_args.bin:   0%|          | 0.00/5.30k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"51d103f439124ff9a9db8269e99f99f4"}},"metadata":{}},{"execution_count":15,"output_type":"execute_result","data":{"text/plain":"CommitInfo(commit_url='https://huggingface.co/tam6/HAR-medication-finetuned/commit/9cd7e903282c41001795d212e66f6ec0a2974967', commit_message='Upload folder using huggingface_hub', commit_description='', oid='9cd7e903282c41001795d212e66f6ec0a2974967', pr_url=None, repo_url=RepoUrl('https://huggingface.co/tam6/HAR-medication-finetuned', endpoint='https://huggingface.co', repo_type='model', repo_id='tam6/HAR-medication-finetuned'), pr_revision=None, pr_num=None)"},"metadata":{}}],"execution_count":15},{"id":"a552eca1-ed3e-4078-9784-9a24c3b7a09c","cell_type":"code","source":"# Load the processor and model from the HuggingFace Hub\nprocessor = AutoImageProcessor.from_pretrained(\"Adekiii/HAR-medication-finetuned\")\nmodel = SiglipForImageClassification.from_pretrained(\"Adekiii/HAR-medication-finetuned\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-16T20:15:30.765430Z","iopub.execute_input":"2025-05-16T20:15:30.768821Z","iopub.status.idle":"2025-05-16T20:15:39.527078Z","shell.execute_reply.started":"2025-05-16T20:15:30.768547Z","shell.execute_reply":"2025-05-16T20:15:39.525454Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"preprocessor_config.json:   0%|          | 0.00/418 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"20df500a56334640ba665cec21a380ad"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/728 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1990189fe994427fb721e88817442d2f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/372M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"dddc732a8d7f4c2087df6195cb9b9ff0"}},"metadata":{}},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_35/4249502878.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m# Load the processor and model from the HuggingFace Hub\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mprocessor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAutoImageProcessor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Adekiii/HAR-medication-finetuned\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSiglipForImageClassification\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Adekiii/HAR-medication-finetuned\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;31m# Load and preprocess your image\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/modeling_utils.py\u001b[0m in \u001b[0;36m_wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    277\u001b[0m         \u001b[0mold_dtype\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_default_dtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    278\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 279\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    280\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    281\u001b[0m             \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_default_dtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mold_dtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/modeling_utils.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, weights_only, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m   4397\u001b[0m                 \u001b[0moffload_index\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4398\u001b[0m                 \u001b[0merror_msgs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4399\u001b[0;31m             \u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_load_pretrained_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4400\u001b[0m                 \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4401\u001b[0m                 \u001b[0mstate_dict\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/modeling_utils.py\u001b[0m in \u001b[0;36m_load_pretrained_model\u001b[0;34m(cls, model, state_dict, checkpoint_files, pretrained_model_name_or_path, ignore_mismatched_sizes, sharded_metadata, device_map, disk_offload_folder, offload_state_dict, dtype, hf_quantizer, keep_in_fp32_regex, device_mesh, key_mapping, weights_only)\u001b[0m\n\u001b[1;32m   4831\u001b[0m             \u001b[0;31m# Skip it with fsdp on ranks other than 0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4832\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mis_fsdp_enabled\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mis_local_dist_rank_0\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mis_quantized\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4833\u001b[0;31m                 disk_offload_index, cpu_offload_index = _load_state_dict_into_meta_model(\n\u001b[0m\u001b[1;32m   4834\u001b[0m                     \u001b[0mmodel_to_load\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4835\u001b[0m                     \u001b[0mstate_dict\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/_contextlib.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    114\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mctx_factory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 116\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    117\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/modeling_utils.py\u001b[0m in \u001b[0;36m_load_state_dict_into_meta_model\u001b[0;34m(model, state_dict, shard_file, expected_keys, reverse_renaming_mapping, device_map, disk_offload_folder, disk_offload_index, cpu_offload_folder, cpu_offload_index, hf_quantizer, is_safetensors, keep_in_fp32_regex, unexpected_keys, device_mesh)\u001b[0m\n\u001b[1;32m    822\u001b[0m                     \u001b[0mparam_device\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"cpu\"\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mis_local_dist_rank_0\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"meta\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    823\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 824\u001b[0;31m                 \u001b[0m_load_parameter_into_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparam_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparam\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam_device\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    825\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    826\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/modeling_utils.py\u001b[0m in \u001b[0;36m_load_parameter_into_model\u001b[0;34m(model, param_name, tensor)\u001b[0m\n\u001b[1;32m    710\u001b[0m     \u001b[0mmodule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparam_type\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_module_from_name\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparam_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    711\u001b[0m     \u001b[0;31m# This will check potential shape mismatch if skipped before\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 712\u001b[0;31m     \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mparam_type\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtensor\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstrict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0massign\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    713\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    714\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mload_state_dict\u001b[0;34m(self, state_dict, strict, assign)\u001b[0m\n\u001b[1;32m   2579\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2580\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merror_msgs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2581\u001b[0;31m             raise RuntimeError(\n\u001b[0m\u001b[1;32m   2582\u001b[0m                 \"Error(s) in loading state_dict for {}:\\n\\t{}\".format(\n\u001b[1;32m   2583\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"\\n\\t\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merror_msgs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mRuntimeError\u001b[0m: Error(s) in loading state_dict for Linear:\n\tsize mismatch for bias: copying a param with shape torch.Size([15]) from checkpoint, the shape in current model is torch.Size([2])."],"ename":"RuntimeError","evalue":"Error(s) in loading state_dict for Linear:\n\tsize mismatch for bias: copying a param with shape torch.Size([15]) from checkpoint, the shape in current model is torch.Size([2]).","output_type":"error"}],"execution_count":7},{"id":"3dae158d-d3f4-4d3f-a511-f46166ccadf7","cell_type":"code","source":"test_data = dataset[\"test\"]\nprint(test_data)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"074d14d6-856e-425b-85a3-da92e992e7bb","cell_type":"code","source":"inputs = processor(images=test_data[\"pixel_values\"], return_tensors=\"pt\")\n\n# Run inference\npredicted_labels = []\ntrue_labels = test_data[\"labels\"]\n\nwith torch.no_grad():\n    #for im in test_data[\"pixel_values\"]\n    outputs = model(**inputs)\n    logits = outputs.logits\n    predicted_class_idx = logits.argmax(-1).item()\n    #predicted_labels.append(predicted_class_idx)\nprecision, recall, f1, _ = precision_recall_fscore_support(true_labels, predicted_class_idx, average=\"binary\", zero_division=0)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"afd3b86a-052c-46f8-b3f9-d57bd0f96786","cell_type":"code","source":"print(\"f1 score on test set: \" + str(f1))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-16T20:29:01.940676Z","iopub.execute_input":"2025-05-16T20:29:01.941348Z","iopub.status.idle":"2025-05-16T20:29:01.989971Z","shell.execute_reply.started":"2025-05-16T20:29:01.941318Z","shell.execute_reply":"2025-05-16T20:29:01.988858Z"}},"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_35/3359312529.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"f1 score on test set: \"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;31mNameError\u001b[0m: name 'f1' is not defined"],"ename":"NameError","evalue":"name 'f1' is not defined","output_type":"error"}],"execution_count":19},{"id":"f91284d9-ebb9-4675-ab0a-a5d775a58119","cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}