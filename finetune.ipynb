{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":31040,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":5,"nbformat":4,"cells":[{"id":"ab21d24a","cell_type":"code","source":"from transformers import SiglipForImageClassification, AutoImageProcessor\nimport os\nimport pandas as pd\nfrom PIL import Image\nfrom datasets import Dataset\nfrom transformers import Trainer, TrainingArguments\nimport numpy as np\nfrom sklearn.metrics import accuracy_score, precision_recall_fscore_support\nimport torch","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"b3440522","cell_type":"code","source":"! apt-get install -y gdown","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"476daad5-a99d-4678-a522-c51345749867","cell_type":"code","source":"# download zip from google drive and unzip contents\n! gdown --id 1rbISVuHbT_AJPw4wv4ywLym3TLVuGLDH      ","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"76737129-e66d-45ee-84ca-65586cd6438d","cell_type":"code","source":"# Unzip the file\nimport zipfile\nwith zipfile.ZipFile(\"./frames_and_annotations.zip\", \"r\") as zip_ref:\n    zip_ref.extractall(\"frames\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"d96a7c86-9c6e-4e49-a717-c9d828f39fba","cell_type":"code","source":"df = pd.read_csv(\"./frames/frames_annotations.csv\")\n\n# fix backslash to forward slash in relative paths\ndf[\"image\"] = df[\"image\"].apply(lambda x: os.path.join(\"frames\", x.replace(\"\\\\\", \"/\")))\n\n#assign labels\ndf[\"label\"] = df[\"label\"].map({\"not_taking_medication\": 0, \"taking_medication\": 1})\n\n# generate dataset from df\ndataset = Dataset.from_pandas(df)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"1b77880b","cell_type":"code","source":"model = SiglipForImageClassification.from_pretrained(\"prithivMLmods/Human-Action-Recognition\")\nprocessor = AutoImageProcessor.from_pretrained(\"prithivMLmods/Human-Action-Recognition\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"6a0ad2c1","cell_type":"code","source":"def preprocess(example):\n    image = Image.open(example[\"image\"]).convert(\"RGB\")\n    inputs = processor(images=image, return_tensors=\"pt\")\n    inputs[\"labels\"] = example[\"label\"]\n    return {\n        \"pixel_values\": inputs[\"pixel_values\"].squeeze(),\n        \"labels\": inputs[\"labels\"]\n    }","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"97101522","cell_type":"code","source":"dataset = dataset.map(preprocess, remove_columns=[\"image\", \"label\"])\ndataset = dataset.train_test_split(test_size=0.1)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"02a5111d","cell_type":"code","source":"def compute_metrics(eval_pred):\n    logits, labels = eval_pred\n    # Ensure logits are numpy arrays\n    if isinstance(logits, tuple):\n        print(\"logits is tuple\")\n        logits = logits[0]\n    if isinstance(logits, torch.Tensor):\n        print(\"logits is tensor\")\n        logits = logits.detach().cpu().numpy()\n    if isinstance(labels, torch.Tensor):\n        print(\"labels is tensor\")\n        labels = labels.detach().cpu().numpy()\n    preds = np.argmax(logits, axis=1)\n    acc = accuracy_score(labels, preds)\n    precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average=\"binary\", zero_division=0)\n    return {\n        \"accuracy\": acc,\n        \"precision\": precision,\n        \"recall\": recall,\n        \"f1\": f1,\n    }","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"2030c48a","cell_type":"code","source":"model.config.label2id = {\"not_taking_medication\": 0, \"taking_medication\": 1}\nmodel.config.id2label = {0: \"not_taking_medication\", 1: \"taking_medication\"}\n\ntraining_args = TrainingArguments(\n    output_dir=\"./HAR-medication-finetuned\",\n    per_device_train_batch_size=8,\n    per_device_eval_batch_size=8,\n    eval_strategy=\"epoch\",\n    save_strategy=\"epoch\",\n    num_train_epochs=3,\n    logging_dir=\"./logs\",\n    logging_steps=10,\n    hub_model_id=\"Adekiii/HAR-medication-finetuned\",\n    report_to=[\"none\"],\n)\n\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=dataset[\"train\"],\n    eval_dataset=dataset[\"test\"],\n    compute_metrics=compute_metrics,\n)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"a5a779f6-8731-4422-bb63-3c71d239a34f","cell_type":"code","source":"# Use GPU if available\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel.to(device)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"263c6e9d","cell_type":"code","source":"trainer.train()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"0aa83ffa","cell_type":"code","source":"trainer.save_model(\"./HAR-med-finetunedv2\")\nprocessor.save_pretrained(\"./HAR-med-finetunedv2\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"c01f732a","cell_type":"code","source":"from huggingface_hub import HfApi\n\naccess_token = \"...\"\n\napi = HfApi(token=access_token)\napi.upload_folder(\n    folder_path=\"HAR-med-finetunedv2\",\n    repo_id=\"tam6/HAR-medication-finetunedv2\", # change with own repo\n    repo_type=\"model\",\n)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"a552eca1-ed3e-4078-9784-9a24c3b7a09c","cell_type":"code","source":"# Load the processor and model from the HuggingFace Hub\nprocessor = AutoImageProcessor.from_pretrained(\"Adekiii/HAR-medication-finetuned\")\nmodel = SiglipForImageClassification.from_pretrained(\"Adekiii/HAR-medication-finetuned\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"3dae158d-d3f4-4d3f-a511-f46166ccadf7","cell_type":"code","source":"test_data = dataset[\"test\"][\"features\"]\nprint(test_data)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"879a268e-657d-4116-837b-35d6b76c44ab","cell_type":"code","source":"print(dataset)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"074d14d6-856e-425b-85a3-da92e992e7bb","cell_type":"code","source":"inputs = processor(images=test_data[\"pixel_values\"], return_tensors=\"pt\")\n\n# Run inference\npredicted_labels = []\ntrue_labels = test_data[\"label\"]\ninputs = test_data[\"pixel_values\"]\nwith torch.no_grad():\n    #for im in test_data[\"pixel_values\"]\n    outputs = model(**inputs)\n    logits = outputs.logits\n    predicted_class_idx = logits.argmax(-1).item()\n    #predicted_labels.append(predicted_class_idx)\nprecision, recall, f1, _ = precision_recall_fscore_support(true_labels, predicted_class_idx, average=\"binary\", zero_division=0)\n","metadata":{"trusted":true,"execution":{"execution_failed":"2025-05-16T21:09:28.731Z"}},"outputs":[],"execution_count":null},{"id":"afd3b86a-052c-46f8-b3f9-d57bd0f96786","cell_type":"code","source":"print(\"f1 score on test set: \" + str(f1))","metadata":{"trusted":true,"execution":{"execution_failed":"2025-05-16T21:09:28.732Z"}},"outputs":[],"execution_count":null},{"id":"f91284d9-ebb9-4675-ab0a-a5d775a58119","cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}